{
    "docs": [
        {
            "location": "/",
            "text": "Deploy AI model at scale! \n\n  \n\n    \n Explore the docs \u00bb \n\n  \n \n\n  \n Our Website \n\n    \u00b7\n  \n Examples in Python \n\n\n\n\n\nMlchain Library for Python\n\u00b6\n\n\nMlchain\n helps AI developers to easily run, deploy and monitor AI models and Workflows without needing software engineering experience.\n\n\nThis Mlchain Python library lets you launch models and do many tasks with Mlchain Platform. \n\n\nLearn more\n\n\nSeamless AI App Deployment\n\u00b6\n\n\nMLChain support you in building your own web-based AI application, where everything is pre-designed \nfor your comfort. You will be able to test your app without cumbersome software engineering work that takes \nlonger than training the data itself. \n\n\nTry our tutorial on \nModel Deployment\n.\n\n\nSharing between clients\n\u00b6\n\n\nMLChain Client allows you to also share your AI model's output with regard to developer's specific input. \nThis uses model that is directly hosted by your web-based app, so there is no need for rebuilding or \nmaking cumbersome APIs.\n\n\nTry our tutorial on \nClient Sharing\n.\n\n\nWorkflow\n\u00b6\n\n\nWorkflow is an independent function of MLChain that allows you to process your function \nin a \n parallel \n or a \n pipeline \n manner. This uses multi thread processing without\nthe need of complex DevOps programming, allowing your app to run multiple tasks 20 - 50 times faster than traditional approach.\n\n\nLearn more about \nWorkflow",
            "title": "Home"
        },
        {
            "location": "/#mlchain-library-for-python",
            "text": "Mlchain  helps AI developers to easily run, deploy and monitor AI models and Workflows without needing software engineering experience.  This Mlchain Python library lets you launch models and do many tasks with Mlchain Platform.   Learn more",
            "title": "Mlchain Library for Python"
        },
        {
            "location": "/#seamless-ai-app-deployment",
            "text": "MLChain support you in building your own web-based AI application, where everything is pre-designed \nfor your comfort. You will be able to test your app without cumbersome software engineering work that takes \nlonger than training the data itself.   Try our tutorial on  Model Deployment .",
            "title": "Seamless AI App Deployment"
        },
        {
            "location": "/#sharing-between-clients",
            "text": "MLChain Client allows you to also share your AI model's output with regard to developer's specific input. \nThis uses model that is directly hosted by your web-based app, so there is no need for rebuilding or \nmaking cumbersome APIs.  Try our tutorial on  Client Sharing .",
            "title": "Sharing between clients"
        },
        {
            "location": "/#workflow",
            "text": "Workflow is an independent function of MLChain that allows you to process your function \nin a   parallel   or a   pipeline   manner. This uses multi thread processing without\nthe need of complex DevOps programming, allowing your app to run multiple tasks 20 - 50 times faster than traditional approach.  Learn more about  Workflow",
            "title": "Workflow"
        },
        {
            "location": "/getstarted/core_concepts/",
            "text": "Core Concepts\n\u00b6\n\n\nThe essential points for understanding and effectively using MLChain can be\ngrouped into the following categories:\n\n\n\n\nML Deployment\n\n\nClient API\n\n\nWorkflow\n\n\n\n\nThis document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation. These sections allows you to understand the inner working of particular functions, along with \nreal life example using our service.\n\n\nML Deployment\n\u00b6\n\n\nSimple Machine Learning model deployment is the central feature of ML Chain library.\nOur ServeModel function allows user to deploy their model without requiring software engineering knowledge.\nWe support Flask and grpc for website hosting.\n\n\nRead More...\n\n\nClient Sharing\n\u00b6\n\n\nThis use our Client Class service that allows users to share their ML models to other users, along with reusing and coming \nback to them every time they want to redeploy their model.\n\n\nRead More...\n\n\nWorkflow\n\u00b6\n\n\nWorkflow is an independent function of MLChain that allows you to process your function \nin a \n parallel \n or a \n pipeline \n manner. This uses multi thread processing without\nthe need of complex DevOps programming, allowing your app to run multiple tasks 20 - 50 times faster than traditional approach.\n\n\nRead More...",
            "title": "Core Concepts"
        },
        {
            "location": "/getstarted/core_concepts/#core-concepts",
            "text": "The essential points for understanding and effectively using MLChain can be\ngrouped into the following categories:   ML Deployment  Client API  Workflow   This document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation. These sections allows you to understand the inner working of particular functions, along with \nreal life example using our service.",
            "title": "Core Concepts"
        },
        {
            "location": "/getstarted/core_concepts/#ml-deployment",
            "text": "Simple Machine Learning model deployment is the central feature of ML Chain library.\nOur ServeModel function allows user to deploy their model without requiring software engineering knowledge.\nWe support Flask and grpc for website hosting.  Read More...",
            "title": "ML Deployment"
        },
        {
            "location": "/getstarted/core_concepts/#client-sharing",
            "text": "This use our Client Class service that allows users to share their ML models to other users, along with reusing and coming \nback to them every time they want to redeploy their model.  Read More...",
            "title": "Client Sharing"
        },
        {
            "location": "/getstarted/core_concepts/#workflow",
            "text": "Workflow is an independent function of MLChain that allows you to process your function \nin a   parallel   or a   pipeline   manner. This uses multi thread processing without\nthe need of complex DevOps programming, allowing your app to run multiple tasks 20 - 50 times faster than traditional approach.  Read More...",
            "title": "Workflow"
        },
        {
            "location": "/getstarted/installation/",
            "text": "Installation\n\u00b6\n\n\nIn your terminal, run the following command in the terminal and install \nMLChain to your computer.This should install ML Chain and all of its dependencies.\n\n\n$ pip install https://techainer-packages.s3-ap-southeast-1.amazonaws.com/mlchain/mlchain-0.0.9h1-py3-none-any.whl",
            "title": "Installation"
        },
        {
            "location": "/getstarted/installation/#installation",
            "text": "In your terminal, run the following command in the terminal and install \nMLChain to your computer.This should install ML Chain and all of its dependencies.  $ pip install https://techainer-packages.s3-ap-southeast-1.amazonaws.com/mlchain/mlchain-0.0.9h1-py3-none-any.whl",
            "title": "Installation"
        },
        {
            "location": "/getstarted/tutorial/",
            "text": "Tutorials are the easiest way to get started with MLChain. Below are the specific\nquickstart tutorial to start building your app with MLChain right away.\n\n\n\n\n\n\nModel Deployment\n\n\n\n\n\n\nClient API\n\n\n\n\n\n\nWorkflow",
            "title": "Tutorial"
        },
        {
            "location": "/Model Deployment/general/",
            "text": "The main component of MLChain Machine Learning Model Deployment is the use of \nthe ServeModel class. This class takes various function that is created by your model\n(defined as class YourModel in the following code) and return the output in a web-based \napp.\n\n\nThis allows you to quickly deploy an app without having to build back-end software engineering \nproducts that might be time-consuming and cumbersome.\n\n\nfrom\n \nmlchain.base\n \nimport\n \nServeModel\n\n\n\nclass\n \nYourModel\n:\n\n    \ndef\n \npredict\n(\nself\n,\ninput\n:\nstr\n):\n\n        \n'''\n\n\n        function return input\n\n\n        '''\n\n        \nreturn\n \ninput\n\n\n\nmodel\n \n=\n \nYourModel\n()\n\n\n\nserve_model\n \n=\n \nServeModel\n(\nmodel\n)\n\n\n\n\n\nTo host the above model, you can simply run the command\n\n\nmlchain run server.py --host localhost --port \n5000\n\n\n\n\n\nand your website should be hosting at \nhttp://localhost:5000\n\n\nAccess full tutorial >>",
            "title": "General"
        },
        {
            "location": "/Model Deployment/tutorial/",
            "text": "ML Chain Model Deployment Tutorial\n\u00b6\n\n\nBuild your first Machine Learning app using MLchain. Through this quickstart tutorial we will\nuse MLchain to build our first ML chain app. \n\n\nContents\n\u00b6\n\n\n\n\nInstallation\n\n\nBuilding our app\n\n\n\n\n1. Installation\n\u00b6\n\n\nFirst, install MLChain:\n\n\n$ pip install https://techainer-packages.s3-ap-southeast-1.amazonaws.com/mlchain/mlchain-0.0.4-py3-none-any.whl\n\n\n\n\n\nNext, \n clone the following repository: \n \nhttps://github.com/trungATtechainer/MLChain-Quick-Start\n\n\nIn this repository, you can find the following files:\n\n\n    /root\n        -> model.pt\n        -> app.py\n        -> \n19\n.png \n# for testing\n\n\n\n\n\nThe \n model.pt \n file contains the model we already trained using the MNIST dataset and Pytorch.\nThe model saved here is a state_dict, meaning we will have to redefine our model during \nour tutorial.\n\n\nThe \n app.py \n file is where we will use to deploy our model.\n\n\n(Optional) For a finished tutorial for reference, find it here: \nhttps://github.com/trungATtechainer/MLChain-Full-Tutorial\n\n\n2. Building our app\n\u00b6\n\n\na. Import the libraries\n\u00b6\n\n\nUpon opening the \n app.py \n files, we will import the following libraries:\n\n\n# pytorch dependencies\n\n\nimport\n \ntorch\n\n\nfrom\n \ntorchvision\n \nimport\n \ntransforms\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.nn.functional\n \nas\n \nF\n\n\n\n# support for image processing\n\n\nfrom\n \nPIL\n \nimport\n \nImage\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n# mlchain libraries\n\n\nfrom\n \nmlchain.base\n \nimport\n \nServeModel\n\n\n\n\n\nThe pytorch libraries are to support our pytorch model, while PIL.Image and numpy are used for web-based application.\nWe only need the ServeModel class from our MLChain library.\n\n\nb. Redefine our model\n\u00b6\n\n\nThis redefined model should be defined the same as your prior model for training.\nThis has been provided for you in the \n app.py \n file. \n\n\n# redefine our model\n\n\nclass\n \nNet\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \n...\n \n# your model architecture\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \n...\n \n# your model's forward function\n\n\n\n\n\nc. Create a model instance for MLChain\n\u00b6\n\n\nFirst, create a model class. This will be our main working area. \n\n\nclass\n \nModel\n():\n\n    \n# define and load our prior model\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \n...\n \n# load our model\n\n\n    \ndef\n \nimage_predict\n(\nself\n,\n \nimg\n:\nnp\n.\nndarray\n):\n\n        \n...\n \n# output function\n\n\n\n\n\nTo begin, we download and import the current state of our model. This should be done under the \n init() \n function.\n\n\n    \n# define our model\n\n    \nself\n.\nmodel\n \n=\n \nNet\n()\n \n# same as our pre-defined model above\n\n\n    \n# load model state_dict\n\n    \nself\n.\nmodel\n.\nload_state_dict\n(\ntorch\n.\nload\n(\n'model.pt'\n))\n\n\n    \n# set model in evaluation mode\n\n    \nself\n.\nmodel\n.\neval\n()\n\n\n\n\n\nWe also define \n transform \n to transform any input images that are uploaded under the init function.\nThis function ensures that images sent to our server are pre-processed before feeding into the neural network.\n\n\n    \nself\n.\ntransform\n \n=\n \ntransforms\n.\nCompose\n([\ntransforms\n.\nGrayscale\n(\nnum_output_channels\n=\n1\n),\n\n                                         \ntransforms\n.\nResize\n(\n28\n),\n\n                                         \ntransforms\n.\nToTensor\n(),\n\n                                         \ntransforms\n.\nNormalize\n((\n0.5\n,),\n \n(\n0.5\n,))])\n\n\n\n\n\nNext, for our \n image_predict() \n function, we simply take an image as the input and return the corresponding\nprediction of our model. This function start with turning our img input (originaly a numpy array) into \na PIL.Image instance. This helps our transform function (defined above) transform the image for our analysis.\n\n\nWe also reshape our image into 4 dimensions tensor as the first value represents the batch_size, which is 1.\n\n\n    \ndef\n \nimage_predict\n(\nself\n,\n \nimg\n:\nnp\n.\nndarray\n):\n\n\n        \n# form an PIL instance from Image\n\n        \nimg\n \n=\n \nImage\n.\nfromarray\n(\nimg\n)\n\n\n        \n# transform image using our defined transformation\n\n        \nimg\n \n=\n \nself\n.\ntransform\n(\nimg\n)\n\n\n        \n# reshape image into 4 - dimensions\n\n        \nimg\n \n=\n \nimg\n.\nview\n(\n1\n,\n \nimg\n.\nshape\n[\n0\n],\n \nimg\n.\nshape\n[\n1\n],\n \nimg\n.\nshape\n[\n2\n])\n\n\n\n\n\nWe can now add prediction and return our final result under \n image_predict() \n:\n\n    \n# predict class using our model\n\n    \nwith\n \ntorch\n.\nno_grad\n():\n\n        \n# forward function\n\n        \npreds\n \n=\n \nself\n.\nmodel\n(\nimg\n)\n\n\n        \n# get maximun value\n\n        \npred\n \n=\n \nnp\n.\nargmax\n(\npreds\n,\n \naxis\n=\n1\n)\n\n\n    \n# return our final result (predicted number)\n\n    \nreturn\n \nint\n(\npred\n)\n\n\n\n\nd. Deploy our model\n\u00b6\n\n\nTo define and return our final result, we use MLchain's provided function\nServeModel to serve our app.\n\n\n# deploying our model\n\n\n# define model\n\n\nmodel\n \n=\n \nModel\n()\n\n\n\n# serve model\n\n\nserve_model\n \n=\n \nServeModel\n(\nmodel\n)\n\n\n\n\n\nFinally, run \n\n\n$ mlchain init\n\n\n\n\n\nin your terminal. This create the \n mlconfig.yalm \n file. \nHere, we will be using flask on our \n app.py \n file on our \n localhost\n, so we fix the file to accommodate our need.\nWe also like to host our app using \n flask \n.\n\n\nOn top of these options, we can host our website to our choices.\n\n\nname\n:\n \nDigit\n-\nRecognizer\n \n# name of service\n\n\nentry_file\n:\n \napp\n.\npy\n \n# python file contains object ServeModel\n\n\nhost\n:\n \nlocalhost\n \n# host service\n\n\nport\n:\n \n5000\n \n# port\n\n\nserver\n:\n \nflask\n \n# option flask or grpc\n\n\nwrapper\n:\n \nNone\n \n# option None or gunicorn\n\n\ncors\n:\n \ntrue\n\n\ngunicorn\n:\n \n# config apm-server if uses gunicorn wrapper\n\n    \ntimeout\n:\n \n60\n\n    \nkeepalive\n:\n \n60\n\n    \nmax_requests\n:\n \n0\n\n    \nthreads\n:\n \n1\n\n    \nworker_class\n:\n \n'gthread'\n\n    \numask\n:\n \n'0'\n\n\n\n\n\n(Optional) Learn more about mlconfig file\n\n\nWhen you're ready, run \n\n\n$ mlchain run\n\n\n\n\n\nin your terminal. This should start a website at \nhttps://localhost:5000\n. Access your app\nby going to [SWAGGER] on the top left of the page. \n\n\n\n\nHere you can find all the routing of your app. Click on the function that you wants to try:\n\n\n\n\nClick try it out:\n\n\n\n\nUpload your image:\n\n\n\n\nTest Image:\n\n\n\nThis is our response: \n\n\n\n\nCongratulation on your first ML app using MLChain.\n\n\nCheck Module Detail >>",
            "title": "Quick Tutorial"
        },
        {
            "location": "/Model Deployment/tutorial/#ml-chain-model-deployment-tutorial",
            "text": "Build your first Machine Learning app using MLchain. Through this quickstart tutorial we will\nuse MLchain to build our first ML chain app.",
            "title": "ML Chain Model Deployment Tutorial"
        },
        {
            "location": "/Model Deployment/tutorial/#contents",
            "text": "Installation  Building our app",
            "title": "Contents"
        },
        {
            "location": "/Model Deployment/tutorial/#1-installation",
            "text": "First, install MLChain:  $ pip install https://techainer-packages.s3-ap-southeast-1.amazonaws.com/mlchain/mlchain-0.0.4-py3-none-any.whl  Next,   clone the following repository:    https://github.com/trungATtechainer/MLChain-Quick-Start  In this repository, you can find the following files:      /root\n        -> model.pt\n        -> app.py\n        ->  19 .png  # for testing   The   model.pt   file contains the model we already trained using the MNIST dataset and Pytorch.\nThe model saved here is a state_dict, meaning we will have to redefine our model during \nour tutorial.  The   app.py   file is where we will use to deploy our model.  (Optional) For a finished tutorial for reference, find it here:  https://github.com/trungATtechainer/MLChain-Full-Tutorial",
            "title": "1. Installation"
        },
        {
            "location": "/Model Deployment/tutorial/#2-building-our-app",
            "text": "",
            "title": "2. Building our app"
        },
        {
            "location": "/Model Deployment/tutorial/#a-import-the-libraries",
            "text": "Upon opening the   app.py   files, we will import the following libraries:  # pytorch dependencies  import   torch  from   torchvision   import   transforms  import   torch.nn   as   nn  import   torch.nn.functional   as   F  # support for image processing  from   PIL   import   Image  import   numpy   as   np  # mlchain libraries  from   mlchain.base   import   ServeModel   The pytorch libraries are to support our pytorch model, while PIL.Image and numpy are used for web-based application.\nWe only need the ServeModel class from our MLChain library.",
            "title": "a. Import the libraries"
        },
        {
            "location": "/Model Deployment/tutorial/#b-redefine-our-model",
            "text": "This redefined model should be defined the same as your prior model for training.\nThis has been provided for you in the   app.py   file.   # redefine our model  class   Net ( nn . Module ): \n     def   __init__ ( self ): \n         ...   # your model architecture \n\n     def   forward ( self ,   x ): \n         ...   # your model's forward function",
            "title": "b. Redefine our model"
        },
        {
            "location": "/Model Deployment/tutorial/#c-create-a-model-instance-for-mlchain",
            "text": "First, create a model class. This will be our main working area.   class   Model (): \n     # define and load our prior model \n     def   __init__ ( self ): \n         ...   # load our model \n\n     def   image_predict ( self ,   img : np . ndarray ): \n         ...   # output function   To begin, we download and import the current state of our model. This should be done under the   init()   function.       # define our model \n     self . model   =   Net ()   # same as our pre-defined model above \n\n     # load model state_dict \n     self . model . load_state_dict ( torch . load ( 'model.pt' )) \n\n     # set model in evaluation mode \n     self . model . eval ()   We also define   transform   to transform any input images that are uploaded under the init function.\nThis function ensures that images sent to our server are pre-processed before feeding into the neural network.       self . transform   =   transforms . Compose ([ transforms . Grayscale ( num_output_channels = 1 ), \n                                          transforms . Resize ( 28 ), \n                                          transforms . ToTensor (), \n                                          transforms . Normalize (( 0.5 ,),   ( 0.5 ,))])   Next, for our   image_predict()   function, we simply take an image as the input and return the corresponding\nprediction of our model. This function start with turning our img input (originaly a numpy array) into \na PIL.Image instance. This helps our transform function (defined above) transform the image for our analysis.  We also reshape our image into 4 dimensions tensor as the first value represents the batch_size, which is 1.       def   image_predict ( self ,   img : np . ndarray ): \n\n         # form an PIL instance from Image \n         img   =   Image . fromarray ( img ) \n\n         # transform image using our defined transformation \n         img   =   self . transform ( img ) \n\n         # reshape image into 4 - dimensions \n         img   =   img . view ( 1 ,   img . shape [ 0 ],   img . shape [ 1 ],   img . shape [ 2 ])   We can now add prediction and return our final result under   image_predict()  :      # predict class using our model \n     with   torch . no_grad (): \n         # forward function \n         preds   =   self . model ( img ) \n\n         # get maximun value \n         pred   =   np . argmax ( preds ,   axis = 1 ) \n\n     # return our final result (predicted number) \n     return   int ( pred )",
            "title": "c. Create a model instance for MLChain"
        },
        {
            "location": "/Model Deployment/tutorial/#d-deploy-our-model",
            "text": "To define and return our final result, we use MLchain's provided function\nServeModel to serve our app.  # deploying our model  # define model  model   =   Model ()  # serve model  serve_model   =   ServeModel ( model )   Finally, run   $ mlchain init  in your terminal. This create the   mlconfig.yalm   file. \nHere, we will be using flask on our   app.py   file on our   localhost , so we fix the file to accommodate our need.\nWe also like to host our app using   flask  .  On top of these options, we can host our website to our choices.  name :   Digit - Recognizer   # name of service  entry_file :   app . py   # python file contains object ServeModel  host :   localhost   # host service  port :   5000   # port  server :   flask   # option flask or grpc  wrapper :   None   # option None or gunicorn  cors :   true  gunicorn :   # config apm-server if uses gunicorn wrapper \n     timeout :   60 \n     keepalive :   60 \n     max_requests :   0 \n     threads :   1 \n     worker_class :   'gthread' \n     umask :   '0'   (Optional) Learn more about mlconfig file  When you're ready, run   $ mlchain run  in your terminal. This should start a website at  https://localhost:5000 . Access your app\nby going to [SWAGGER] on the top left of the page.    Here you can find all the routing of your app. Click on the function that you wants to try:   Click try it out:   Upload your image:   Test Image:  This is our response:    Congratulation on your first ML app using MLChain.  Check Module Detail >>",
            "title": "d. Deploy our model"
        },
        {
            "location": "/Model Deployment/moduleDetail/",
            "text": "class\n \nServeModel\n(\nobject\n):\n\n    \ndef\n \n__init__\n(\nself\n, \nmodel\n, \nname\n=\nNone\n, \ndeny_all_function\n=\nFalse\n, \n    \nblacklist\n=[], \nwhitelist\n=[]):\n\n\n\n\nVariables:\n\u00b6\n\n\n\n\n\n\nmodel (instance): The model instance that you defined, including general architecture and \naccompanying function\n\n\n\n\n\n\ndeny_all_function (bool): Do not return any route, except for those in whitelist\n(default: False)\n\n\n\n\n\n\nblacklist (list): list of functions that are not used. Use with deny_all_function == False.\n\n\n\n\n\n\nwhitelist (list): list of functions that are always used. Use with deny_all_function == True.\n\n\n\n\n\n\nExample use:\n\u00b6\n\n\nCase 1: deny_all_function == False, blacklist == [], whitelist == []\n\n\n# serve model\n\n\nserve_model\n \n=\n \nServeModel\n(\nmodel\n)\n\n\n\n\n\nreturn:\n\n\n\nCase 2: deny_all_function == False, blacklist == ['blacklist1', 'blacklist2']\n\n\n# serve model\n\n\nserve_model\n \n=\n \nServeModel\n(\nmodel\n,\n \nblacklist\n \n=\n \n[\n'model'\n,\n \n'transform'\n])\n\n\n\n\n\nreturn: \n\n\n\nCase 3: deny_all_function == True, whitelist == ['model', 'image_predict']\n\n\n# serve model\n\n\nserve_model\n \n=\n \nServeModel\n(\nmodel\n,\n \ndeny_all_function\n \n=\n \nTrue\n,\n \nwhitelist\n \n=\n \n[\n'model'\n,\n \n'image_predict'\n])\n\n\n\n\n\nreturn:\n\n\n\nClient Sharing >>",
            "title": "Module Overview"
        },
        {
            "location": "/Model Deployment/moduleDetail/#variables",
            "text": "model (instance): The model instance that you defined, including general architecture and \naccompanying function    deny_all_function (bool): Do not return any route, except for those in whitelist\n(default: False)    blacklist (list): list of functions that are not used. Use with deny_all_function == False.    whitelist (list): list of functions that are always used. Use with deny_all_function == True.",
            "title": "Variables:"
        },
        {
            "location": "/Model Deployment/moduleDetail/#example-use",
            "text": "Case 1: deny_all_function == False, blacklist == [], whitelist == []  # serve model  serve_model   =   ServeModel ( model )   return:  Case 2: deny_all_function == False, blacklist == ['blacklist1', 'blacklist2']  # serve model  serve_model   =   ServeModel ( model ,   blacklist   =   [ 'model' ,   'transform' ])   return:   Case 3: deny_all_function == True, whitelist == ['model', 'image_predict']  # serve model  serve_model   =   ServeModel ( model ,   deny_all_function   =   True ,   whitelist   =   [ 'model' ,   'image_predict' ])   return:  Client Sharing >>",
            "title": "Example use:"
        },
        {
            "location": "/Model Deployment/mlconfig/",
            "text": "MLConfig File\n\u00b6\n\n\nMLChain config file (mlconfig.yaml) is a powerful tool you can use to automate \nand drive various serve model in MLChain. It sits at the root directory of your \nproject folder (directory where you run \"mlchain init\"). The config file should\nbe written in standard YAML syntax.\n\n\nProvide default arguments for mlchain run\n\u00b6\n\n\nInstead of having to type out all the command arguments everytime you want to serve model from MLChain CLI, you can save a set of default arguments in mlconfig.yaml. With this, all you have to type next time is just \nmlchain run\n.\n\n\nFor example, the following command:\n\n\nmlchain run server.py --host localhost --port \n5000\n --flask --gunicorn\n\n\n\n\nCan be simplified to just \nmlchain run\n if you have the following mlconfig.yml created inside project root directory:\n\n\nname\n:\n \nmlchain-server\n \n# name of service\n\n\nentry_file\n:\n \nserver.py\n \n# python file contains object ServeModel\n\n\nhost\n:\n \nlocalhost\n \n# host service\n\n\nport\n:\n \n5000\n \n# port service\n\n\nserver\n:\n \nflask\n \n# option flask or grpc\n\n\nwrapper\n:\n \ngunicorn\n \n# option None or gunicorn\n\n\ngunicorn\n:\n \n# config apm-server if uses gunicorn wrapper\n\n  \ntimeout\n:\n \n60\n\n  \nkeepalive\n:\n \n60\n\n  \nmax_requests\n:\n \n0\n\n  \nthreads\n:\n \n1\n\n  \nworker_class\n:\n \n'gthread'\n\n  \numask\n:\n \n'0'\n\n\n\n\n\nYou can also override the arguments in \nmlchain run\n command to quickly test out a change. For example:\n\n\nmlchain run --host \n4000\n\n\n\n\n\nis equivalent to the following with above mentioned config file:\n\n\nmlchain run server.py --host localhost --port \n4000\n --flask --gunicorn",
            "title": "Config file"
        },
        {
            "location": "/Model Deployment/mlconfig/#mlconfig-file",
            "text": "MLChain config file (mlconfig.yaml) is a powerful tool you can use to automate \nand drive various serve model in MLChain. It sits at the root directory of your \nproject folder (directory where you run \"mlchain init\"). The config file should\nbe written in standard YAML syntax.",
            "title": "MLConfig File"
        },
        {
            "location": "/Model Deployment/mlconfig/#provide-default-arguments-for-mlchain-run",
            "text": "Instead of having to type out all the command arguments everytime you want to serve model from MLChain CLI, you can save a set of default arguments in mlconfig.yaml. With this, all you have to type next time is just  mlchain run .  For example, the following command:  mlchain run server.py --host localhost --port  5000  --flask --gunicorn  Can be simplified to just  mlchain run  if you have the following mlconfig.yml created inside project root directory:  name :   mlchain-server   # name of service  entry_file :   server.py   # python file contains object ServeModel  host :   localhost   # host service  port :   5000   # port service  server :   flask   # option flask or grpc  wrapper :   gunicorn   # option None or gunicorn  gunicorn :   # config apm-server if uses gunicorn wrapper \n   timeout :   60 \n   keepalive :   60 \n   max_requests :   0 \n   threads :   1 \n   worker_class :   'gthread' \n   umask :   '0'   You can also override the arguments in  mlchain run  command to quickly test out a change. For example:  mlchain run --host  4000   is equivalent to the following with above mentioned config file:  mlchain run server.py --host localhost --port  4000  --flask --gunicorn",
            "title": "Provide default arguments for mlchain run"
        },
        {
            "location": "/Client/general/",
            "text": "Introduction\n\u00b6\n\n\nMLChain class Client allows you to pass your Machine Learning model's output seamlessly between different \ncomputers, servers, and so on.\n\n\nThe below example uses MLChain client to make request from \nhttp://localhost:5000\n, that is hosted by ourselves. \nIn real examples, this can be any website url which contain our model.\n\n\nTutorial\n\u00b6\n\n\nFirst, follow the tutorial \nhere\n and deploy your model at \nhttp://localhost:5000\n, if you haven't already done so.\n\n\nNext, create a file \n client.py \n any where on your computer. At the same time,\ndownload \n \n this \n \n image to that folder.\n\n\n(Optional) For a finished tutorial for reference, find it here: \nhttps://github.com/trungATtechainer/MLChain-Full-Tutorial\n\n\nIn the \n client.py \n file, include the following code:\n\n\nfrom\n \nmlchain.client\n \nimport\n \nClient\n\n\nimport\n \ncv2\n\n\n\nimage_name\n \n=\n \n'19.png'\n \n# downloaded image\n\n\n\ndef\n \nget_num\n(\nimage_name\n):\n\n    \n# tell the system to use the model currently hosting in localhost, port 5000\n\n    \nmodel\n \n=\n \nClient\n(\napi_address\n=\n'localhost:5000'\n,\nserializer\n=\n'json'\n)\n.\nmodel\n(\ncheck_status\n=\nFalse\n)\n\n\n    \n# import our image\n\n    \nimg\n \n=\n \ncv2\n.\nimread\n(\nimage_name\n)\n\n\n    \n# get model response on image\n\n    \nres\n \n=\n \nmodel\n.\nimage_predict\n(\nimg\n)\n\n\n    \n# print result\n\n    \nreturn\n \nres\n\n\n\nif\n \n__name__\n \n==\n \n'__main__'\n:\n\n    \nprint\n(\nget_num\n(\nimage_name\n))\n\n\n\nYou can see that in this code, first we get the name of the image that we downloaded \n 19.png \n as the image\nthat we want to test for digit recognizer. Next, we tell the \n Client \n class to use the model specified at localhost:5000, where\nwe are already hosting our model.\nNext, we read the image, and pass it through the image_prediction function that we wrote \nhere\n to get out final\nresponse, which is the number 4.\n\n\nIn your terminal, running \n\n\n$ python client.py\n\n\n\n\n\nwill return \"res\" as the model's response. Please ensure that you are already hosting your \napp at localhost:5000 using the model deployment feature.\n\n\nThis results in the output of \n\n\n4\n\n\n\n\n\nIn software development, we believe using this service allows programmers to better transfer AI models' final results and allowing \nfor more cooperation between programmers. This allows you to communicate without having to build complex API systems in the company.\n\n\nSending requests to the REST API\n\u00b6\n\n\nYou can also send request to this API using the terminal.\n\n\ncurl -F \n\"img=@19.png\"\n  http://localhost:5000/call/image_predict\n\n\n\n\nIn the above example, we are having a request to the url \nhttp://localhost:5000/call/image_predict\n, \nwhere our input form is our image under variable \n img \n (19.png). This doesn't require you to build\na separate \n client.py \n file.\n\n\nThis also results in the output of \n\n\n{\n\"output\"\n:\n \n4\n,\n \n\"time\"\n:\n \n0.0\n}\n\n\n\n\n\nWhich is our model's response for the image.",
            "title": "General and Tutorial"
        },
        {
            "location": "/Client/general/#introduction",
            "text": "MLChain class Client allows you to pass your Machine Learning model's output seamlessly between different \ncomputers, servers, and so on.  The below example uses MLChain client to make request from  http://localhost:5000 , that is hosted by ourselves. \nIn real examples, this can be any website url which contain our model.",
            "title": "Introduction"
        },
        {
            "location": "/Client/general/#tutorial",
            "text": "First, follow the tutorial  here  and deploy your model at  http://localhost:5000 , if you haven't already done so.  Next, create a file   client.py   any where on your computer. At the same time,\ndownload     this     image to that folder.  (Optional) For a finished tutorial for reference, find it here:  https://github.com/trungATtechainer/MLChain-Full-Tutorial  In the   client.py   file, include the following code:  from   mlchain.client   import   Client  import   cv2  image_name   =   '19.png'   # downloaded image  def   get_num ( image_name ): \n     # tell the system to use the model currently hosting in localhost, port 5000 \n     model   =   Client ( api_address = 'localhost:5000' , serializer = 'json' ) . model ( check_status = False ) \n\n     # import our image \n     img   =   cv2 . imread ( image_name ) \n\n     # get model response on image \n     res   =   model . image_predict ( img ) \n\n     # print result \n     return   res  if   __name__   ==   '__main__' : \n     print ( get_num ( image_name ))  \nYou can see that in this code, first we get the name of the image that we downloaded   19.png   as the image\nthat we want to test for digit recognizer. Next, we tell the   Client   class to use the model specified at localhost:5000, where\nwe are already hosting our model.\nNext, we read the image, and pass it through the image_prediction function that we wrote  here  to get out final\nresponse, which is the number 4.  In your terminal, running   $ python client.py  will return \"res\" as the model's response. Please ensure that you are already hosting your \napp at localhost:5000 using the model deployment feature.  This results in the output of   4   In software development, we believe using this service allows programmers to better transfer AI models' final results and allowing \nfor more cooperation between programmers. This allows you to communicate without having to build complex API systems in the company.",
            "title": "Tutorial"
        },
        {
            "location": "/Client/general/#sending-requests-to-the-rest-api",
            "text": "You can also send request to this API using the terminal.  curl -F  \"img=@19.png\"   http://localhost:5000/call/image_predict  In the above example, we are having a request to the url  http://localhost:5000/call/image_predict , \nwhere our input form is our image under variable   img   (19.png). This doesn't require you to build\na separate   client.py   file.  This also results in the output of   { \"output\" :   4 ,   \"time\" :   0.0 }   Which is our model's response for the image.",
            "title": "Sending requests to the REST API"
        },
        {
            "location": "/Client/moduleDetail/",
            "text": "class Client(ClientBase):\n\n\n    \"\"\"\n\n\n    Mlchain Client Class\n\n\n    \"\"\"\n\n\n    def __init__(self, api_address = None, serializer='json')\n\n\n\n\n\nVariables:\n\u00b6\n\n\n\n\n\n\napi_address (str): Website URL where the current ML model is hosted\n\n\n\n\n\n\nserializer (str): 'json' or 'msgpack' package types where the ML model data is returned",
            "title": "Module Detail"
        },
        {
            "location": "/Client/moduleDetail/#variables",
            "text": "api_address (str): Website URL where the current ML model is hosted    serializer (str): 'json' or 'msgpack' package types where the ML model data is returned",
            "title": "Variables:"
        },
        {
            "location": "/workflow/general/",
            "text": "Workflow is an independent function of MLChain that allows you to process your function \nin a \n parallel \n or a \n pipeline \n manner. This uses multi thread processing without\nthe need of complex DevOps programming, allowing your app to run multiple tasks at your CPU's best ability.\n\n\nParallel Processing\n\u00b6\n\n\n Parallel \n processing are designed specifically to speed up your task. For instance, if you are\nhaving a task (eg. image classification) that needs to be done 20 times, using \n parallel \n with \n20 threads speed that up 20 times, meaning it only take as long as processing 1 image.\n\n\nView full tutorial >>\n\n\nPipeline Processing\n\u00b6\n\n\n Pipeline \n processing are designed to speed up your work when multiple tasks are required to be done.\nFor instance, if you are building an OCR system, it will have to be broken down into multiple processes \n(eg. text localization, text segmentation, text recognition). If you are processing 10 image, for instance, it will take\nall of them to be processed through all these steps, drastically increase time. \n Pipeline\n\n uses multi threads to better allocate those time, so that while you are processing \n text segmentation \n on image 1,\nfor instance, then the computer can already begin processing \n text localization \n on image 2, and so on.\n\n\nView full tutorial >>",
            "title": "General"
        },
        {
            "location": "/workflow/general/#parallel-processing",
            "text": "Parallel   processing are designed specifically to speed up your task. For instance, if you are\nhaving a task (eg. image classification) that needs to be done 20 times, using   parallel   with \n20 threads speed that up 20 times, meaning it only take as long as processing 1 image.  View full tutorial >>",
            "title": "Parallel Processing"
        },
        {
            "location": "/workflow/general/#pipeline-processing",
            "text": "Pipeline   processing are designed to speed up your work when multiple tasks are required to be done.\nFor instance, if you are building an OCR system, it will have to be broken down into multiple processes \n(eg. text localization, text segmentation, text recognition). If you are processing 10 image, for instance, it will take\nall of them to be processed through all these steps, drastically increase time.   Pipeline  uses multi threads to better allocate those time, so that while you are processing   text segmentation   on image 1,\nfor instance, then the computer can already begin processing   text localization   on image 2, and so on.  View full tutorial >>",
            "title": "Pipeline Processing"
        },
        {
            "location": "/workflow/parallel/",
            "text": "Parallel \n processing are designed specifically to speed up your task. For instance, if you are\nhaving a task (eg. image classification) that needs to be done 20 times, using \n parallel \n with \n20 threads speed that up 20 times, meaning it only take as long as processing 1 image.\n\n\nTutorial\n\u00b6\n\n\n(Optional) For a finished tutorial for reference, find it here: \nhttps://github.com/trungATtechainer/MLChain-Full-Tutorial\n\n\nLet's use our \n get_num() \n function from \nclient.py\n for this task. After creating \na new file, called \n parallel.py\n in the same directory, we import the important libraries. \n\n\n# import mlchain workflows library\n\n\nfrom\n \nmlchain.workflows\n \nimport\n \nParallel\n,\nTask\n\n\n\n# import time for measurement purposes\n\n\nimport\n \ntime\n\n\n\n# import get_num function from client.py\n\n\nfrom\n \nclient\n \nimport\n \nget_num\n\n\n\nNext, download the data from \n \nhere\n \n and save into the same directory, under a folder called \n data\n. They contain the images that we will use\nfor our digit classification task. This should have been satisfied if you followed the \n \npipeline\n \n guide.\n\n\nAfter downloading the images, we create a name list of images that we want to process.\n\n\n# create list of images\n\n\nimg_list\n \n=\n \n[\n'17.png'\n,\n \n'18.png'\n,\n \n'30.png'\n,\n \n'31.png'\n,\n \n'32.png'\n,\n \n'41.png'\n,\n \n            \n'44.png'\n,\n \n'46.png'\n,\n \n'51.png'\n,\n \n'55.png'\n,\n \n'63.png'\n,\n \n'68.png'\n,\n\n            \n'76.png'\n,\n \n'85.png'\n,\n \n'87.png'\n,\n \n'90.png'\n,\n \n'93.png'\n,\n \n'94.png'\n,\n\n            \n'97.png'\n,\n \n'112.png'\n,\n \n'125.png'\n,\n \n'137.png'\n,\n \n'144.png'\n,\n \n            \n'146.png'\n]\n \n# contains 24 images\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\nlen\n(\nimg_list\n)):\n\n    \nimg_list\n[\ni\n]\n \n=\n \n'data/'\n \n+\n \nimg_list\n[\ni\n]\n \n# specify PATH toward images\n\n\n\n\n\nTest 1 (for loop)\n\u00b6\n\n\nIn this test, we will test the programming using the good old for loop. In your file, paste the following code.\n\n\nstart\n \n=\n \ntime\n.\ntime\n()\n \n# start time\n\n\n\n# running a for loop through all the tasks\n\n\nr\n \n=\n \n[]\n\n\nfor\n \nitem\n \nin\n \nimg_list\n:\n\n    \nr\n.\nappend\n(\nget_num\n(\nitem\n))\n\n\nend\n \n=\n \ntime\n.\ntime\n()\n\n\n\n# printing results and processed time\n\n\nprint\n \n(\nr\n)\n\n\nprint\n(\n'Total time: '\n,\n \nend\n \n-\n \nstart\n)\n\n\n\nIn this code, we basically run image classification for all of our images. Once that is done, it prints out\nthe result and the time taken.\n\n\n[\n8\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n]\n\n\nTotal\n \ntime\n:\n  \n48.9379518032074\n\n\n\n\n\nThis process took us up to 49 seconds to run, which is unfavourable in most software development context.\nNow, let's see how long it will take to run with parallel.\n\n\nTest 2 (run with parallel)\n\u00b6\n\n\nRemember to \n comment \n the previous code that we have written for test 1. Add the following to the \n parallel.py \n file:\n\n\nstart\n \n=\n \ntime\n.\ntime\n()\n \n# start time\n\n\n\n# Using Parallel\n\n\nr\n \n=\n \nParallel\n(\n\n    \ntasks\n=\n \n[\nTask\n(\nget_num\n,\ni\n)\n \nfor\n \ni\n \nin\n \nimg_list\n],\n \n# listing the tasks\n\n    \nmax_threads\n=\n24\n \n# no. of threads. 24 threads for 24 images to minimize run time\n\n\n)\n.\nrun\n()\n\n\n\nend\n \n=\n \ntime\n.\ntime\n()\n \n# end time\n\n\n\n# printing result and time\n\n\nprint\n(\nr\n)\n\n\nprint\n(\n'Total time: '\n,\n \nend\n \n-\n \nstart\n)\n\n\n\n\n\nIn this code, we are using parallel to run our program. This is specified by the \n Parallel() \n instance above. \nIn which, we specify the tasks that we want the computer to run, which is \n get_num() \n\nfor all images in img_list.\n\n\n[\n8\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n3\n,\n \n3\n,\n \n3\n,\n \n8\n,\n \n8\n,\n \n3\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n]\n\n\nTotal\n \ntime\n:\n  \n2.1767709255218506\n\n\n\n\n\nThis process takes only approximately \n 2 seconds \n which is 24 times faster than our original \ntest 1. This is an advantage of mlchain workflow parallel, by allowing users to \noptimize their code without having DevOps knowledge.\n\n\nModule overview:\n\u00b6\n\n\nclass\n \nParallel\n:\n\n    \ndef\n \n__init__\n(\nself\n,\n \ntasks\n:[],\n \nmax_threads\n:\nint\n=\n10\n,\n \nmax_retries\n:\nint\n=\n0\n,\n \n                \npass_fail_job\n:\nbool\n=\nFalse\n,\n \nverbose\n:\nbool\n=\nTrue\n,\n \nthreading\n:\nbool\n=\nTrue\n):\n\n\n\n\n\nVariables:\n\u00b6\n\n\n\n\n\n\ntasks (list): list of tasks (or functions) that you want to be completed\n\n\n\n\n\n\nmax_threads (int): maximum number of threads that the computer is allowed to use (default = 10)\n\n\n\n\n\n\nmax_retries (int):  (default = 0)\n\n\n\n\n\n\npass_fail_job (bool):  (default = False)\n\n\n\n\n\n\nverbose (bool):  (default = True)\n\n\n\n\n\n\nthreading (bool):  (default = True)",
            "title": "Parallel"
        },
        {
            "location": "/workflow/parallel/#tutorial",
            "text": "(Optional) For a finished tutorial for reference, find it here:  https://github.com/trungATtechainer/MLChain-Full-Tutorial  Let's use our   get_num()   function from  client.py  for this task. After creating \na new file, called   parallel.py  in the same directory, we import the important libraries.   # import mlchain workflows library  from   mlchain.workflows   import   Parallel , Task  # import time for measurement purposes  import   time  # import get_num function from client.py  from   client   import   get_num  \nNext, download the data from    here    and save into the same directory, under a folder called   data . They contain the images that we will use\nfor our digit classification task. This should have been satisfied if you followed the    pipeline    guide.  After downloading the images, we create a name list of images that we want to process.  # create list of images  img_list   =   [ '17.png' ,   '18.png' ,   '30.png' ,   '31.png' ,   '32.png' ,   '41.png' ,  \n             '44.png' ,   '46.png' ,   '51.png' ,   '55.png' ,   '63.png' ,   '68.png' , \n             '76.png' ,   '85.png' ,   '87.png' ,   '90.png' ,   '93.png' ,   '94.png' , \n             '97.png' ,   '112.png' ,   '125.png' ,   '137.png' ,   '144.png' ,  \n             '146.png' ]   # contains 24 images  for   i   in   range ( len ( img_list )): \n     img_list [ i ]   =   'data/'   +   img_list [ i ]   # specify PATH toward images",
            "title": "Tutorial"
        },
        {
            "location": "/workflow/parallel/#test-1-for-loop",
            "text": "In this test, we will test the programming using the good old for loop. In your file, paste the following code.  start   =   time . time ()   # start time  # running a for loop through all the tasks  r   =   []  for   item   in   img_list : \n     r . append ( get_num ( item ))  end   =   time . time ()  # printing results and processed time  print   ( r )  print ( 'Total time: ' ,   end   -   start )  \nIn this code, we basically run image classification for all of our images. Once that is done, it prints out\nthe result and the time taken.  [ 8 ,   3 ,   3 ,   8 ,   3 ,   8 ,   3 ,   8 ,   3 ,   8 ,   3 ,   3 ,   3 ,   8 ,   3 ,   3 ,   3 ,   8 ,   8 ,   3 ,   8 ,   8 ,   8 ,   8 ]  Total   time :    48.9379518032074   This process took us up to 49 seconds to run, which is unfavourable in most software development context.\nNow, let's see how long it will take to run with parallel.",
            "title": "Test 1 (for loop)"
        },
        {
            "location": "/workflow/parallel/#test-2-run-with-parallel",
            "text": "Remember to   comment   the previous code that we have written for test 1. Add the following to the   parallel.py   file:  start   =   time . time ()   # start time  # Using Parallel  r   =   Parallel ( \n     tasks =   [ Task ( get_num , i )   for   i   in   img_list ],   # listing the tasks \n     max_threads = 24   # no. of threads. 24 threads for 24 images to minimize run time  ) . run ()  end   =   time . time ()   # end time  # printing result and time  print ( r )  print ( 'Total time: ' ,   end   -   start )   In this code, we are using parallel to run our program. This is specified by the   Parallel()   instance above. \nIn which, we specify the tasks that we want the computer to run, which is   get_num()  \nfor all images in img_list.  [ 8 ,   3 ,   3 ,   8 ,   3 ,   8 ,   3 ,   8 ,   3 ,   8 ,   3 ,   3 ,   3 ,   8 ,   3 ,   3 ,   3 ,   8 ,   8 ,   3 ,   8 ,   8 ,   8 ,   8 ]  Total   time :    2.1767709255218506   This process takes only approximately   2 seconds   which is 24 times faster than our original \ntest 1. This is an advantage of mlchain workflow parallel, by allowing users to \noptimize their code without having DevOps knowledge.",
            "title": "Test 2 (run with parallel)"
        },
        {
            "location": "/workflow/parallel/#module-overview",
            "text": "class   Parallel : \n     def   __init__ ( self ,   tasks :[],   max_threads : int = 10 ,   max_retries : int = 0 ,  \n                 pass_fail_job : bool = False ,   verbose : bool = True ,   threading : bool = True ):",
            "title": "Module overview:"
        },
        {
            "location": "/workflow/parallel/#variables",
            "text": "tasks (list): list of tasks (or functions) that you want to be completed    max_threads (int): maximum number of threads that the computer is allowed to use (default = 10)    max_retries (int):  (default = 0)    pass_fail_job (bool):  (default = False)    verbose (bool):  (default = True)    threading (bool):  (default = True)",
            "title": "Variables:"
        },
        {
            "location": "/workflow/pipeline/",
            "text": "Pipeline \n processing are designed to speed up your work when multiple tasks are required to be done.\nFor instance, if you are building an OCR system, it will have to be broken down into multiple processes \n(eg. text localization, text segmentation, text recognition). If you are processing 10 image, for instance, it will take\nall of them to be processed through all these steps, drastically increase time. \n Pipeline\n\n uses multi threads to better allocate those time, so that while you are processing \n text segmentation \n on image 1,\nfor instance, then the computer can already begin processing \n text localization \n on image 2, and so on.\n\n\nTutorial\n\u00b6\n\n\n(Optional) For a finished tutorial for reference, find it here: \nhttps://github.com/trungATtechainer/MLChain-Full-Tutorial\n\n\nLet's use our get_num() function from \nclient.py\n for this task. After creating a new file,\ncalled pipeline.py in the same directory, we import the important libraries.\n\n\nFirst, import the necessary libraries. \n\n\n# mlchain Pipeline \n\n\nfrom\n \nmlchain.workflows.pipeline\n \nimport\n \nPipeline\n,\nStep\n\n\n\n# time for measurement purposes\n\n\nimport\n \ntime\n\n\n\n# import get_num function from client.py\n\n\nfrom\n \nclient\n \nimport\n \nget_num\n\n\n\n\n\nNext, download the data from \n \nhere\n \n and save into the same directory, under a folder called \n data\n. They contain the images that we will use\nfor our digit classification task. This should have been satisfied if you followed the \n \nparallel\n \n guide.\n\n\nAfter downloading the images, we create a name list of images that we want to process.\n\n\n# create list of images\n\n\nimg_list\n \n=\n \n[\n'17.png'\n,\n \n'18.png'\n,\n \n'30.png'\n,\n \n'31.png'\n,\n \n'32.png'\n,\n \n'41.png'\n,\n \n            \n'44.png'\n,\n \n'46.png'\n,\n \n'51.png'\n,\n \n'55.png'\n,\n \n'63.png'\n,\n \n'68.png'\n,\n\n            \n'76.png'\n,\n \n'85.png'\n,\n \n'87.png'\n,\n \n'90.png'\n,\n \n'93.png'\n,\n \n'94.png'\n,\n\n            \n'97.png'\n,\n \n'112.png'\n,\n \n'125.png'\n,\n \n'137.png'\n,\n \n'144.png'\n,\n \n            \n'146.png'\n]\n \n# contains 24 images\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\nlen\n(\nimg_list\n)):\n\n    \nimg_list\n[\ni\n]\n \n=\n \n'data/'\n \n+\n \nimg_list\n[\ni\n]\n \n# specify PATH toward images\n\n\n\n\n\nWe also create another function \n get_square()\n, that takes the input x and return its square. \nFor demonstration purpose, we introduce \n time.sleep(1) \n in this function. This will represents \nanother AI function, for instance, that takes approximately 1 second after the first, original digit recognition step.\n\n\ndef\n \nget_square\n(\nx\n):\n\n    \ntime\n.\nsleep\n(\n1\n)\n\n    \nreturn\n \nx\n**\n2\n\n\n\n\n\nTest 1: Using for loop\n\u00b6\n\n\nIn this test, we will test the programming using the good old for loop. In your file, paste the following code.\n\n\n# traditional approach\n\n\nstart\n \n=\n \ntime\n.\ntime\n()\n \n#start time\n\n\n\n# run a for loop through both function and return result\n\n\nr\n \n=\n \n[]\n\n\nfor\n \nitem\n \nin\n \nimg_list\n:\n\n    \nnumber\n \n=\n \nget_num\n(\nitem\n)\n \n# get number from image (~2s)\n\n    \nsquare_num\n \n=\n \nget_square\n(\nnumber\n)\n \n# get square (~1s)\n\n    \nr\n.\nappend\n(\nsquare_num\n)\n\n\n\nend\n \n=\n \ntime\n.\ntime\n()\n \n# end time\n\n\n\nprint\n(\nr\n)\n \n# print results\n\n\nprint\n(\n'Total time: '\n,\n \nend\n \n-\n \nstart\n)\n\n\n\n\n\nThis code basically goes through the two functions above (\nget_num()\n and \nget_square()\n) for each image at a time.\n\n\n[\n64\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n64\n,\n \n64\n,\n \n64\n]\n\n\nTotal\n \ntime\n:\n  \n72.85201978683472\n\n\n\n\n\nAs predicted, since each image takes 2s to determine the digit and an additional 1 \nsecond to process \n get_square()\n, it takes us a total of ~72 seconds to run.\n\n\nNow, let's do the same thing using \n pipeline \n\n\nTest 2: Using pipeline\n\u00b6\n\n\nRemember to \n # comment \n the previous code that we have written for test 1. Add \nthe following to the \n pipeline.py \n file:\n\n\nstart\n \n=\n \ntime\n.\ntime\n()\n \n# start time\n\n\n\n# pipeline architecture\n\n\npipeline\n \n=\n \nPipeline\n(\n\n    \nStep\n(\nget_num\n,\n \nmax_thread\n \n=\n \n24\n),\n\n    \nStep\n(\nget_square\n,\n \nmax_thread\n \n=\n \n12\n)\n\n\n)\n\n\n\n#print results\n\n\nr\n \n=\n \npipeline\n(\n*\nimg_list\n)\n \n# get results (* since input has to be multiple values)\n\n\nend\n \n=\n \ntime\n.\ntime\n()\n \n# end time\n\n\n\nprint\n(\nr\n)\n\n\nprint\n(\n'Total time: '\n,\n \nend\n \n-\n \nstart\n)\n\n\n\n\n\nThis code takes the class \n pipeline \n to run our program. As we passes through class \n pipeline \n \nthe different steps for the get_num() and get_square() function, it can begin to process on \n image 2 \n\neven if we haven't finished processing \n image 1. \n\n\n[\n64\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n9\n,\n \n9\n,\n \n9\n,\n \n64\n,\n \n64\n,\n \n9\n,\n \n64\n,\n \n64\n,\n \n64\n,\n \n64\n]\n\n\nTotal\n \ntime\n:\n  \n3.1787729263305664\n\n\n\n\n\nThis \n pipeline \n class thus allows you to build your products more effectively.\n\n\nModule overview:\n\u00b6\n\n\nclass\n \nPipeline\n:\n\n    \ndef\n \n__init__\n(\nself\n,\n \n*\nsteps\n:\n \nStep\n):\n\n\n\n\n\nVariables:\n\u00b6\n\n\n\n\n*steps (Step instances): list of steps we want to execute for each instance of input.",
            "title": "Pipeline"
        },
        {
            "location": "/workflow/pipeline/#tutorial",
            "text": "(Optional) For a finished tutorial for reference, find it here:  https://github.com/trungATtechainer/MLChain-Full-Tutorial  Let's use our get_num() function from  client.py  for this task. After creating a new file,\ncalled pipeline.py in the same directory, we import the important libraries.  First, import the necessary libraries.   # mlchain Pipeline   from   mlchain.workflows.pipeline   import   Pipeline , Step  # time for measurement purposes  import   time  # import get_num function from client.py  from   client   import   get_num   Next, download the data from    here    and save into the same directory, under a folder called   data . They contain the images that we will use\nfor our digit classification task. This should have been satisfied if you followed the    parallel    guide.  After downloading the images, we create a name list of images that we want to process.  # create list of images  img_list   =   [ '17.png' ,   '18.png' ,   '30.png' ,   '31.png' ,   '32.png' ,   '41.png' ,  \n             '44.png' ,   '46.png' ,   '51.png' ,   '55.png' ,   '63.png' ,   '68.png' , \n             '76.png' ,   '85.png' ,   '87.png' ,   '90.png' ,   '93.png' ,   '94.png' , \n             '97.png' ,   '112.png' ,   '125.png' ,   '137.png' ,   '144.png' ,  \n             '146.png' ]   # contains 24 images  for   i   in   range ( len ( img_list )): \n     img_list [ i ]   =   'data/'   +   img_list [ i ]   # specify PATH toward images   We also create another function   get_square() , that takes the input x and return its square. \nFor demonstration purpose, we introduce   time.sleep(1)   in this function. This will represents \nanother AI function, for instance, that takes approximately 1 second after the first, original digit recognition step.  def   get_square ( x ): \n     time . sleep ( 1 ) \n     return   x ** 2",
            "title": "Tutorial"
        },
        {
            "location": "/workflow/pipeline/#test-1-using-for-loop",
            "text": "In this test, we will test the programming using the good old for loop. In your file, paste the following code.  # traditional approach  start   =   time . time ()   #start time  # run a for loop through both function and return result  r   =   []  for   item   in   img_list : \n     number   =   get_num ( item )   # get number from image (~2s) \n     square_num   =   get_square ( number )   # get square (~1s) \n     r . append ( square_num )  end   =   time . time ()   # end time  print ( r )   # print results  print ( 'Total time: ' ,   end   -   start )   This code basically goes through the two functions above ( get_num()  and  get_square() ) for each image at a time.  [ 64 ,   9 ,   9 ,   64 ,   9 ,   64 ,   9 ,   64 ,   9 ,   64 ,   9 ,   9 ,   9 ,   64 ,   9 ,   9 ,   9 ,   64 ,   64 ,   9 ,   64 ,   64 ,   64 ,   64 ]  Total   time :    72.85201978683472   As predicted, since each image takes 2s to determine the digit and an additional 1 \nsecond to process   get_square() , it takes us a total of ~72 seconds to run.  Now, let's do the same thing using   pipeline",
            "title": "Test 1: Using for loop"
        },
        {
            "location": "/workflow/pipeline/#test-2-using-pipeline",
            "text": "Remember to   # comment   the previous code that we have written for test 1. Add \nthe following to the   pipeline.py   file:  start   =   time . time ()   # start time  # pipeline architecture  pipeline   =   Pipeline ( \n     Step ( get_num ,   max_thread   =   24 ), \n     Step ( get_square ,   max_thread   =   12 )  )  #print results  r   =   pipeline ( * img_list )   # get results (* since input has to be multiple values)  end   =   time . time ()   # end time  print ( r )  print ( 'Total time: ' ,   end   -   start )   This code takes the class   pipeline   to run our program. As we passes through class   pipeline   \nthe different steps for the get_num() and get_square() function, it can begin to process on   image 2  \neven if we haven't finished processing   image 1.   [ 64 ,   9 ,   9 ,   64 ,   9 ,   64 ,   9 ,   64 ,   9 ,   64 ,   9 ,   9 ,   9 ,   64 ,   9 ,   9 ,   9 ,   64 ,   64 ,   9 ,   64 ,   64 ,   64 ,   64 ]  Total   time :    3.1787729263305664   This   pipeline   class thus allows you to build your products more effectively.",
            "title": "Test 2: Using pipeline"
        },
        {
            "location": "/workflow/pipeline/#module-overview",
            "text": "class   Pipeline : \n     def   __init__ ( self ,   * steps :   Step ):",
            "title": "Module overview:"
        },
        {
            "location": "/workflow/pipeline/#variables",
            "text": "*steps (Step instances): list of steps we want to execute for each instance of input.",
            "title": "Variables:"
        },
        {
            "location": "/help/",
            "text": "You can find more FAQs, tutorials and articles in the \nHelp Center\n.\n\n\nAlso, check the \ncommunity Forum\n, this is another great resource for common questions, use-cases and feature requests.\n\n\nNothing of above worked?! \nDon't worry\n! We are happy to help you at \nadmin@techainer.com\n.",
            "title": "Need Help?"
        }
    ]
}